from dotenv import load_dotenv
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import time


# åŒä¸€ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()
# ãƒ¢ãƒ‡ãƒ«ã¯Gemini-1.5-flashã‚’æŒ‡å®š
llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')

def get_lecture_title(file_path):
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æˆæ¥­ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    """
    if file_path.endswith(".pdf"):
        n = file_path.replace(".pdf", "")
    elif file_path.endswith(".docx"):
        n = file_path.replace("_aud.docx", "")
    # ã‚‚ã—nã«audãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€æˆæ¥­ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ãªã„
    if "aud" in n:
        return f"ç¬¬{n}å›è¬›ç¾©éŒ²éŸ³"
    else:
        return f"ç¬¬{n}å›è¬›ç¾©è³‡æ–™"

@st.cache_resource
def load_embeddings_and_index():
    embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")
    index = FAISS.load_local("faiss_index/", embeddings, allow_dangerous_deserialization=True)
    return index.as_retriever(search_type="similarity", search_kwargs={"k": 10})


if "knowledge_base" not in st.session_state:
    open_know_st = time.time()
    retriever = load_embeddings_and_index()
    open_know_end = time.time()
    print(f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿æ™‚é–“: {open_know_end - open_know_st:.2f}ç§’")
else:
    print("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã¨ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’å–å¾—")

retriever = st.session_state.retriever

# åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
system_message = SystemMessage(content="ã‚ãªãŸã¯æˆæ¥­ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n\
                                        æˆæ¥­ã®å†…å®¹ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\
                                        è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆã¯ã€å‚è€ƒæƒ…å ±ã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\
                                        å‚è€ƒè³‡æ–™ã§ã€pdfã¨docxã§åŒã˜æ•°å­—ãŒãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã¯ã€åŒã˜å†…å®¹ã«ã¤ã„ã¦è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\
                                        ã‚‚ã—ã€å‚è€ƒæƒ…å ±ã«é–¢ä¿‚ã™ã‚‹å˜èªãŒå…¨ããªã„å ´åˆã¯ã€é–¢ä¿‚ã™ã‚‹å˜èªãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\
                                        ã‚‚ã—ã€å°‘ã—ã ã‘é–¢é€£ã™ã‚‹å˜èªãŒã‚ã‚‹å ´åˆã¯ã€é–¢é€£ã™ã‚‹å˜èªã‚’ä½¿ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n")

                                        
# ä¼šè©±å±¥æ­´ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
# ã®ã¡ã«ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã¨ãã€ä¼šè©±å±¥æ­´ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ã®ã‚’é˜²ããŸã‚ã«ã€
# session_stateã«conversation_historyã‚’æ ¼ç´
# session_stateã¯ã€Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®è¾æ›¸å‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
# æœ€åˆã ã‘åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = [system_message]
conversation_history = st.session_state.conversation_history

# Streamlitã®UIã‚’ä½œæˆ
st.title("çŸ¥èƒ½æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ -QAChatBot")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ä¼šè©±å±¥æ­´ã‚’ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹messagesã‚’ä½œæˆ
# messagesã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨AIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã‚‹
# chat_inputã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã‚‹ãŸã‚ã®Streamlitã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
prompt = st.chat_input("ãŠå›°ã‚Šã®ã“ã¨ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„ã€‚")

if prompt:
    pro_st = time.time()
    st.session_state.messages.append({"role": "user", "content": prompt})
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã£ãŸã‚‰ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å–å¾—
    print("get_retriever")
    retriever_st = time.time()
    context_text = "\n\n".join([
                                f"[{doc.metadata.get('source')}]\n{doc.page_content}" 
                                for doc in retriever.invoke(prompt)]) # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å–å¾—
    print("get_context_text")
    context_st = time.time()
    print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æ™‚é–“: {context_st - retriever_st:.2f}ç§’")
    gen_prompt = f"è³ªå•: {prompt}\n\nä»¥ä¸‹ã¯ã€å‚è€ƒæƒ…å ±ã§ã™ã€‚\n\n{context_text}\n" 
    context_end = time.time()
    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆæ™‚é–“: {context_end - context_st:.2f}ç§’")
    st.session_state.information = f"ä»¥ä¸‹ã¯ã€å‚è€ƒæƒ…å ±ã§ã™ã€‚\n{context_text}"
    conversation_history.append(HumanMessage(content=gen_prompt)) # ä¼šè©±å±¥æ­´ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã¨å‚è€ƒæƒ…å ±ã‚’è¿½åŠ 


    with st.chat_message("user"):
        st.markdown(prompt) # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’è¡¨ç¤º
    with st.chat_message("assistant"):
        response_st = time.time()
        response = llm.invoke(conversation_history) # ãƒ¢ãƒ‡ãƒ«ã«ä¼šè©±å±¥æ­´ã‚’æ¸¡ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
        response_end = time.time()
        print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_end - response_st:.2f}ç§’")
        # ä¼šè©±å±¥æ­´ã«AIã®å¿œç­”ã‚’è¿½åŠ 
        conversation_history.append(AIMessage(content=response.content))
        st.markdown(response.content) # AIã®å¿œç­”ã‚’è¡¨ç¤º
    st.session_state.messages.append({"role": "assistant", "content": response.content}) 

# å‚è€ƒæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ãƒœã‚¿ãƒ³ã‚’ä½œæˆ
# if promptã®å¤–ã«è¡¨ç¤ºã—ãªã„ã¨ã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã‚‚æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œãªã„
if st.button("å‚è€ƒæƒ…å ±ã‚’è¡¨ç¤º"):
    if "information" in st.session_state:
        st.markdown(st.session_state.information)

if st.button("ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"):
    st.cache_resource.clear()
    st.experimental_rerun()

